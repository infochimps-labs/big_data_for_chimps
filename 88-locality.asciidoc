

* Time horizon infinite; converges to truth; scroll
* Combiner within initial compute horizon; group and combine
* Window dimension is a horizon dimension; hard windows
  - strictly nested panes corresponding to horizon dimensions
* Sorted






==== Batch, Window, Partition and Group

Trident has a few essential ways in which it segments tuples into non-overlapping sets:

* _batch_     -- the unit of transactional state at the transport level. Often abused as an effective event-order window at the data level.
* _partition_ -- a bounded set of tuples guaranteed to be processed by the same executors in partial order
* _group_     -- data-level division into substreams each having the same group key. Each group lies within exactly one partition

To get the rest of the pig relations, you will want to at some point introduce two more segmentations:

* _subgroup_  -- an optional secondary sort within a group defining further hierarchical subgrouping. This captures ["data cube" operations](http://arnab.org/blog/introducing-cube-operator-apache-pig)
http://web.cecs.pdx.edu/~tufte/papers/StreamSemantics.pdf

* _window_    -- 

* Scroll -- continuously sliding window -- advances tuple-by-tuple.

Bracket -- punctuation carrying aggregates on a windowed range.


Synthesizing a new proposition
observation -- values for dimensions at point in time; ¿Data? -- beliefs held to be durably true; 

* Volume of Justification - 
* 
* Timescale of stability (beliefs to become untrustworthy over time).
* Synthesis
    * set of propositions needed to assemble new belief
    * A window is explicitly what you want: "count requests, errors and logins by minute", or "every ten seconds emit the average request rate over the trailing 60 seconds, or "rebin by"; or it could be a compromise.
* Single horizon of computation -- an indication that the stream is finite. Every Hadoop job has an implicit window the size of the data sent to its tasks.
* range of computation / conversation: number of events it's convenient to do bulk requests against remote data store
* Intermediate aggregation: group/batch/parallelism
* Horizon of belief: the smallest amount of data you're willing to synthesize a new understanding.
* Range of computational risk
    * cost of retry
    * eventual consistency rate: commit needed for truth to settle
* Range of acceptable delay: if your stream may suffer disorder of up to five minutes, an infinite bandwidth zero latency flow can't converge your belief less than five minutes in worst case.

Trident should support the concept of a _stream window_, implemented using 'punctuation' tuples.

Many CEP (Complex Event Processing) systems offer this concept

* _scrolling_ --
* _window_
* _window and pane_ -- 



landmark, tumbling, slide-by-tuple, partitioned, etc.

* are tuples nested?















=== Three legs of the big data stack

In early 2012, my company (Infochimps) began selling our big data stack to enterprise companies. At first, clients came to us with the word Hadoop on their lips and a flood of data on their hands -- yet what consistently emerged as the right solution was an initial implementation using streaming data analytics into a scalable datastore, and a follow-on installment of Hadoop once their application reached scale. From nowhere on their radar last year, we now hear requests for Storm by name. I've seen Hadoop's historically fast growth from open-source product with momentum in 2008 to foundational enterprise technology in 2013, and can attest that the rate of adoption for streaming analytics (and Storm+Trident in particular) looks to be even faster. 

It's become clear that a big data application platform should have three legs: streaming analytics, to process records as they are created; one or more scalable databases, for processing records as they are consumed; and batch processing, for results that require the full dataset. 

The workflow described in this book unifies and simplifies the streaming data and Hadoop frameworks without limiting their fundamental power. We use wukong (Ruby) for direct transformations of records, and high-level DSLs (Pig for Hadoop, Trident for streaming) to orchestrate structural transformations of datasets. This lets the data scientist focus on their data and problem domain, not on the low-level complexity of the half-dozen APIs otherwise involved.

For both Storm+Trident and Hadoop, our intent is to demonstrate

1. a pattern language for orchestrating the global structural changes efficiently,
2. practical instruction and street-fighting techniques for data science as it's done
3. real-data, real-problem case studies to demonstrate the above

These overlap far more than you might expect, and the hard part reamins the same: understanding 'locality' and how to deal with data dispersed across hundreds of machines. (In fact, a map-reduce job is simply a certain type of especially bursty dataflow.) Storm+Trident introduces a second DSL for describing that process, but the hard part is what script to write, not the writing of the script. The chapter on statistics will simplify in scope but describe both global and streaming algorithms for statistical summaries. The chapter on time series algorithms will be scaled back to simply being anomaly detection ("trending topics" detection), and presented using Storm+Trident. Third, we'll repurpose material from the log processing and the machine learning chapters to demonstrate an end-to-end big data application that combines Hadoop, Storm+Trident and HBase to make efficient online recommendations for a large-scale web application. The major additions are a chapter on tuning and on the internals of Storm+Trident.

==== Batch, Window, Partition and Group

your data is now comprehensive -- _everything_ about something, no longer a sample -- which is awesome. But getting insight still isn't easy. Getting signal from noise -- vs getting signal from signal.

* Some Overwhelming Practical considerations
* Unreasonable effectiveness of big data: 
* When basis volume of belief won't fit into horizon of computation -- Approximation, algorithm, method
* 


Trident has a few essential ways in which it segments tuples into non-overlapping sets:

* _batch_     -- the unit of transactional state at the transport level. Often abused as an effective event-order window at the data level.
* _partition_ -- a bounded set of tuples guaranteed to be processed by the same executors in partial order
* _group_     -- data-level division into substreams each having the same group key. Each group lies within exactly one partition

To get the rest of the pig relations, you will want to at some point introduce two more segmentations:

* _subgroup_  -- an optional secondary sort within a group defining further hierarchical subgrouping. This captures ["data cube" operations](http://arnab.org/blog/introducing-cube-operator-apache-pig)
http://web.cecs.pdx.edu/~tufte/papers/StreamSemantics.pdf

* _window_    -- 

* Scroll -- continuously sliding window -- advances tuple-by-tuple.

Bracket -- punctuation carrying aggregates on a windowed range.



There are three 

* A window is explicitly what you want: "count requests, errors and logins by minute", or "every ten seconds emit the average request rate over the trailing 60 seconds, or "rebin by"
* Single horizon of computation -- an indication that the stream is finite. Every Hadoop job has an implicit window the size of the data sent to its tasks.
* Unit of computation 


Let's talk about Data Frames and Tidy Data. The proper data warehousing term for it is "third normal form" Hadley Wickham has coined the term "tidy data".

In 
* Every row is an observation
* Every column is distinct (never two values in a column) and uniform (all have same type)
* Every file (table) has one data type (set of columns)

A table with the address smushed together ("742 Evergreen Terrace, Springfield") might in some cases be distinct-columned, but it's likely that the value should be split into address and city (if not street number, street and city).

The time axis here is not exactly 3NF(?)

* The 'time' axis is the principal (row) axis 
* can bracket a partition
* ....

Temporal sorting:

Each of the following is enough to enable you to sort a slightly-disordered stream using finite memory.
* _prompt_           -- every event is received within no more than time `D` delay. 
* _ordered_	     -- events arrive in time order (though with arbitrarily large delay -- ordered doesn't imply prompt, and prompt doesn't imply ordered).
* _partial-ordered_  -- the stream can be non-overlappingly segmented into ordered substreams. You can depend that the sixth record from webserver-A will come after the fifth and before the seventh record from that webserver. Every partial-ordered stream is also ordered, and Trident will preserve the partial-order of a stream within a partition.
* _block-disordered_ -- events will arrive out of sequence, but there is a bounded block horizon guaranteeing all events in one block are received before any event in a future block. My laundry-folding scheme is block disordered: I process a basket of jumbled-together socks, then jumbled-together shirts, then jumbled-together towels. There is no shirt that is folded up after a towel has been folded up, because each basket comes from the dryer in a separate load.
* _band-disordered_  -- there is some statistical bound (i.e. every record is received within 50 slots of its natural order, or 99.9% of records are received within 50 slots of natural order)
* _punctuated_	     -- a special tuple introduced into the stream that will always occur last in its segment. Trident uses a $coord tuple punctuation to delimit each batch partition. More generally, a punctuation is a pattern rule guaranteeing that no tuple matching the rule occurs after it in the stream. For example `m = webserver-7, t < 2013-04-26T12:00:00Z` means that all windows ending before that day noon can safely process the records from webserver-7.)

* types of windows: landmark, tumbling, slide-by-tuple, partitioned, etc.

Given those,

If you then add

* _arbitrarily large sorted buffers_
* _data-local execution on large blobs_ -- if you run HDFS datanodes on all the storm worker machines, and can specify that a 

I would set the default assumption that no changes to the core grammar are necessary: it will help enforce abstraction. For the particular examples you chose: PARALLEL should correspond directly. LOAD and STORE are perfectly reasonable verbs to use for Trident spouts and partitionPersist operations. I don't know whether a datastore-backed persistentAggregate should correspond to a STORE, or should instead be an annotation on the various relation verbs. Implement that on a memory-backed state only and see what happens in practice.



           ^^
           ||
      group / subgroup
           ||
           vv

      <----|----|----|---- process ordering
      <--/\_/\_-^--v- temporal ordering

An aggregator for sales data subgrouped on `(["product"], ["location", "year"], ["sales"])` would receive records with three fields (location, year, sales), each having the same value for product, and in order of location-then-year.

exclusively covering (by which I mean "set partition", but partition is too over-loaded).

12 10 9 11 8 7 6 5 3 4 2 14 10
      12 10 9 11 8 7 6 5 3 4 13 12 11

==== Locality Models

I'm using "Pivot" as a verb. You do combinations of transforms -- manipulations of data elements on their own -- and pivots -- large-scale orchestration to put data elements in context, to bring them to the same place and time.

I may have figured out a better word, but it needs work:

"relativity".

You prepare data in place with transformations. Here are some transformations:

* reject all voter file records that lack a zip code
* take a set of records having (county, representative, all congressional districts in county) and emit (congressional district, county, representative)
* take a large set of comma-separated strings; parse each one and construct a tidy data object with well-chosen names and uniform data types.

...and then perform operations that relate data elements to each other. Here are relations:

* "group" -- prepare sets of voter records, each holding all voters having the same zip code
* "cogroup" -- prepare sets of (voter record, donation history, volunteer sheet card), one for each zip code, with all relevent records from each of the three sets
* "sort" -- put all the voter records in order by last name
* "decorate" -- for each voter file record that lacks a zip code, look up its street+city+state in a remote database

("Relation" is actually the term of art for these things, though I'm abusing the name slightly)

The distinction is important because Relations care whether data is "local" -- each of them requires arranging the data into a certain context. So another way I can express this concept is to say that if the data is not arranged suitably its records are non-relative; the outcome of a Relation is that related records are ready to be locally transformed.



Proximity - adjacency
Context - reshape - pivot


* RPC - RPC
* Client-server data store
* Streaming Analytics
* Fabric (VCD)
* Batch

* Latency
* Throughput
* Tempo -- how often does data change?
* Size -- how large is record?
* Access control -- security; API rate limits
* Data model -- your web log hit (with path, response time, HTTP status code, etc) is my sales lead.

==== Lambda Architecture

* _Fast data_: recorded live, updates allowed with partial locality or denormalized data
* _Slow data_: gold data, using global data, full answer.


Data is an _observation of a set of named facts_ taken _at a given point in time_. We will organize those within named _topics_ -- streams of records with similar structure ad meaning

Change of address form example
Why not just store and retrieve all? a) simpliity of query-side code b) efficiency c) source domain model tyranny d) locality.

* Identifiers
* Immutable Ground Truth(?)
* Mutable Ground Truth
* Immutable observation
* Consistent Summary
* Approximate Summary
* Idempotent Synthesis
* Identifier reconciliation

Weather data: weather stations take immutable observations of atm'c vars, artificial identifier, immutable ground truth of weather location. Weather-by-hour-and-station is idempotent synthesis (when done in batch) or consistent summary (done live).

==== Example lambda architecture: product rating aggregator

* Products have model numbers, names, attributes and prices
    * 
* Vendors 
    * some vendors: bulk upload of inventory. this is mutable ground truth, so we can update with clobber
* Raters
* Ratings
* Tweets, incl sentiment
    - count mentions by product name

The core value of your product is a clear, unified exploration of different sites. If products or deals show up multiple times in searches, and inconsistent information is scattered across incomplete pages, users will derive no value from the site

On the other hand, timeliness is also key. I'm writing this before the event, but I confidently predict that the release of "Big Data for Chimps" will set the whole twittersphere abuzz, with glowing reviews from Shaq and Lady Gaga. It's better to have several transiently inconsistent records 

==== Architecture

* Collection layer -- spouts that dispense opaque blobs
* Parse layer -- turn blob into data structure that corresponds to source data model
* Extraction layer -- produce activity model
* Summary layer -- combine activity model to summarized model and persist to backing store (note: the "summary" might be a no-op)



===== Why can't you just do it all in the stream?

The law of small numbers holds here -- in a data stream of billions of events, there are thousands of one-in-a-million anomalies.

Master data reconciliation is a classic "Neighbor's lawnmower" problem -- gee, it sure does look easy to fix from over here across the street, maybe I should ask Bill if he remembered to put gas in the tank.

There are existing records A: `<name: "stapler", upc:12345| ...>`, B:`<best_buy_id:23, walmart_id:69>` and C:`<mfr_id:8675309, amz_id:42| ...>`, each with associated fields. A batch of records arrives, including ones that assert D:`<mfr_id:8675309, best_buy_id:23>` and E:`<upc:12345, walmart_id:69>`. With our global perspective in hand, it's clear all of these record pertain to the same product. In the stream, however, there's no prior way to recognize that D and E should be grouped together. One reaction is to say "well, query an indentifier reconciliation table, update it and then group." However many reconciliation stages you spackle on, as more identifiers are added to the dataflow you'll need another. It's common to have dozens, hundreds or thousands of matching keys in a real-world master data management dataflow. Now throw in the fact that these records will be infuriatingly inconsistent, even to the point of making conflicting assertions about their hard identifiers. 

You can handle the problem consistently in Hadoop, because you have the whole world in your hands. Freeze time and make locality pivoting easy, and can make the reconciliation logic arbitrarily sophisticated

The point is not to repair the flaws in this naïve approach. It's that there's little value in doing so.

It's primarily a practical question
It's slightly harder than you think, your code will be tangibly more complex and unpredictable than you think, and the business value of a good answer produced slowly will outweigh the value of a slightly less bad answer produced quickly.

* Make a processor that accepts `<[unified profile], [{new tidbit}, {new tidbit}, ...], [{relevant prior record}, {relevant prior record}]>`
* Given a set of ground truths or faithful summaries, idempotently synthesizes a unified consensus record. 


IF your 

==== Example lambda architecture: online pagerank

* Start with stable pagerank.
* When a new node is discovered, just "borrow" a notional pagerank allocation from its neighbors
* Don't worry about any beyond immediate locality
* Later, batch job re-settles the graph.
* Pagerank calculation is idempotent: within reason, any perturbed input will settle out.

==== locality in stream

* GroupBy / Partitioned aggregates
* DRPC
* Denormalized remote data request
* Hash join -- hold a cached version of table and decorate

===== Why can you get away with 

Storm/Trident has buffering and throttling mechanisms built in

Hadoop is designed to drive all system resources to their full limit until the fundamental limiting resource is encountered. 

==== Why Storm+Trident is bigger than it looks


*  Operational decoupling:
* Latency Tolerance:
* Reliability Glue:
* Transport Agnosticism:
* Distributed Programming without quantum mechanics

How do you make a program that will run forever? Joe Armstrong, the inventor of Erlang, identifies these six key features: 
Isolation; Concurrency; Failure Detection; Fault Identification, Live Code Upgrade; Stable Storage
Storm+Trident provides all six, 
