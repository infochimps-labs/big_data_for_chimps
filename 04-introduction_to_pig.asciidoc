[[intro_to_pig]]
== Introduction to Pig

// === Olga, the Remarkable Calculating Pig
//
// JT and Nanette were enjoying the rising success of C&E Corp. The translation and SantaCorp projects were in full production, and they'd just closed two more deals that closely resembled the SantaCorp gig.
//
// Still, it was quite a thrill when the manager for Olga the Remarkable Calculating Pig reached out to _them_, saying Olga had a proposition to discuss. Imagine! The star that played nightly to sell-out crowds at Carnegie Hall, whose exploits of numeracy filled the journals and whose exploits of romance filled the tabloids, working with JT and Nanette! "Why don't you kids come see the show -- we'll leave tickets for you at the gate -- and you can meet with Olga after she gets off."
//
// That night they watched, spellbound, as Olga performed monstrous feats of calculation and recall. In one act, she tallied the end-of-year accounting reports for three major retailers while riding a unicycle; in another, she listed the box-office numbers for actors whose names were drawn from a hat. Needless to say, the crowd roared for more, JT and Nanette along with them. For the grand finale, a dozen audience members wrote down their favorite baseball players -- most well-known, but of course some wise guy wrote down Alamazoo Jennings, Snooks Dowd or Vinegar Bend Mizell to be intentionally obscure footnote:[Yes, these are names of real major league baseball players.]. Olga not only recited the complete career stats for every one, but the population of their hometown; every teammate they held in common; and the construction date of every stadium they played in.
//
// "I tell you, that's some pig", Nanette said to JT as they waited outside the dressing rooms. "Terrific," JT agreed. A voice behind them said "Radiant and Humble, too, they tell me." They turned to find Olga, now dressed in street clothes. "Why don't you join me for a drink? We can talk then."
//
// === Nanette and Olga Have an Idea
//
// Once settled in at a bar down the street, JT broke the ice. "Olga, your show was amazing. When you rattled off Duluth, Minnesota's daily low and high temperatures from 1973 to 1987, chills ran down my spine. But I can't quite figure out what kind of help C&E Corp can provide for you?" Nanette chimed in, "Actually, I think I have an idea -- but I'd like to hear your thoughts first, Olga."
//
// As Olga explained, "I first heard about you from my publisher -- my friend Charlotte and I wrote a book about web crawlers, and thanks to your work we're selling as many copies overseas as we are domestically. But it wasn't until I visited the translation floor that I really appreciated the scale of content you guys were moving. And that's what I'm looking for -- high scale.
//
// "You might know that besides my stage act I consult on the side for companies who need a calculating animal savant. I love that just as much as being on stage, but the fact is that what I can do for my clients just seems so _limited_. I've got insurance companies who want to better judge tornado risk so they can help people protect their homes; but to do this right means using the full historical weather data. I have to explain to them that I'm just one pig -- I'd melt down if I tried to work with that much information.
//
// "Goldbug automakers engages me to make inventory reports based on daily factory output and dealership sales, and I can literally do this in my sleep. But they're collecting thousands of times that much data each second. For instance, they gather status reports from every automated step in their factory. If I could help Goldbug compare the manufacturing data of the cars as they're built to the maintenance records of those cars after sale, we'd be able to find patterns in the factory that match warranty claims down the road. Predicting these manufacturing defects early would enable my client to improve quality, profit and customer satisfaction.
//
// "I wish I could say I invited you for this drink because I knew the solution, but all I have is a problem I'd like to fix. I know your typewriter army helps companies process massive amounts of documents, so you're used to working with the amount of information I'm talking about. Is the situation hopeless, or can you help me find a way to apply my skills at a thousand times the scale I work at now?"
//
// Nanette smiled. "It's not hopeless at all, and to tell you the truth your proposal sounds like the other end of a problem I've been struggling with.
//
// "We've now had several successful client deliveries, and recently JT's made some breakthroughs in what our document handling system can do -- it involves having the chimpanzees at one set of typewriters send letters to another set of chimpanzees at a different set of typewriters. One thing we're learning is that even though the actions that the chimpanzees take are different for every client, there are certain themes in how the chimpanzees structure their communication that recur across clients.
//
// "Now JT here" (at this, JT rolled his eyes for effect, as he knew what was coming) "spent all his time growing up at a typewriter, and so he thinks about information flow as a set of documents. Designing a new scheme for chimpanzees to send inter-office memos is like pie for him. But where JT thinks about working with words on a page, I think about managing books and libraries. And the other thing we're learning is that our clients think like me. They want to be able to tell us the big picture, not fiddly little rules about what should happen to each document. Tell me how you describe the players-and-stadiums trick you did in the grand finale.
//
// "Well, I picture in my head the teams every player was on for each year they played, and at the same time a listing of each team's stadium by year. Then I just think 'match the players\' seasons to the teams\' seasons using the team and year', and the result pops into my head.
//
// Nanette nodded and looked over at JT. "I see what you're getting at now," he replied. "In my head I'm thinking about the process of matching individual players and stadiums -- when I explain it you're going to think it sounds more complicated but I don't know, to me it seems simpler. I imagine that I could ask each player to write down on a yellow post-it note the team-years they played on, and ask each stadium manager to write down on blue post-it notes the team-years it served. Then I put those notes in piles -- whenever there's a pile with yellow post-it notes, I can read off the blue post-it notes it matched.
//
// Nanette leaned in. "So here's the thing. Elephants and Pigs have amazing memories, but not Chimpanzees -- JT can barely keep track of what day of the week it is. JT's scheme never requires him to remember anything more than the size of the largest pile -- in fact, he can get by with just remembering what's on the yellow post-it notes. But
//
// "Well," Nanette said with a grin, "Pack a suitcase with a very warm jacket. We're going to take a trip up north -- way north."

// TODO fix up story

// Olga is playing nightclubs, just what she can do in her head. Wants to start doing stadiums
// she knows the patterns -- the vaudeville routines. (Make JT blush with one)
// question is how to translate the small-venue act to massive scale.
// 22 panels that always work http://momentofcerebus.blogspot.com/2012/07/wally-woods-22-panels-that-always-work.html
// find people with the same birthday, complete tax reforms of all attendees,
// (farrowed - litter of 10 piglets.) no, keep Pig as the thing coordinating a map/reduce flow.
//

// ch 5 baseball: score keeping?
// ch 6 pig performance, show group
// ch 7 joining
// ch 8


=== Pig Helps Hadoop work with Tables, not Records

Apache Pig is an open-source, high-level language that enables you to create efficient Map/Reduce jobs using clear, maintainable scripts. Its interface is similar to SQL, which makes it a great choice for folks with significant experience there. It’s not identical, though, and things that are efficient in SQL may not be so in Pig (we will try to highlight those traps).

Let’s dive in with an example using the UFO dataset to estimate whether aliens tend to visit in some months over others:

.Pig - UFO Sightings by Month (ch_03/ufos_by_month.pig)
------
sightings = LOAD '/data/gold/geo/ufo_sightings/ufo_sightings.tsv'  AS (
    sighted_at: chararray,   reported_at: chararray,    location_str: chararray, shape: chararray,
    duration_str: chararray, description: chararray,    lng: float,              lat: float,
    city: chararray,         county: chararray,         state: chararray,        country: chararray );

-- Take the 6th and 7th character from the original string, as in '2010-06-25T05:00:00Z', take '06'
month_count = FOREACH sightings GENERATE SUBSTRING(sighted_at, 5, 7) AS month;

-- Group by year_month, and then count the size of the 'bag' this creates to get a total
ufos_by_month    = FOREACH (GROUP month_count BY month) GENERATE
  group AS month, COUNT_STAR(month_count) AS total;
STORE ufos_by_month INTO './ufos_by_month.out';
------

In a Python streaming or traditional Hadoop job, the focus is on the record and you’re best off thinking in terms of message passing or grouping. In Pig, the focus is much more on the structure and you should think in terms of relational and set operations. In the example above, each line described an operation on the full dataset; we declared what change to make and Pig, as you’ll see, executes those changes by dynamically assembling and running a set of Map/Reduce jobs.

To run the Pig job, go into the example code repository and run:

------
pig examples/ch_03/ufos_by_month.pig
------

If you consult the Job Browser, you should a single Map/Reduce jobs; the dataflow Pig instructed Hadoop to run is essentially similar to the Python script you ran. What Pig ran was, in all respects, a Hadoop job. It calls on some of Hadoop’s advanced features to help it operate but nothing you could not access through the standard Java API.

To see the result of the Pig script:

------
hadoop fs -cat ufos_by_month.out/*
------

Which will result in:

------
01	4263
02	3644
03	4170
04	4120
05	4220
06	6745
07	7361
08	6641
09	5665
10	5421
11	4954
12	3933
	256
------

Note that 256 records had no such value - data will often surprise you. You might filter these empty values, or look closer at the raw data to see what is going on. Also, did you notice the output was sorted? That is no coincidence -- as you saw in Chapter 2, Hadoop sorted the results in order to group them. Sorting in this case is free! We'll learn how to explicitly sort data in Pig in a future chapter.

image::images/images/03-first-pig-run.png["First Pig Script Run, and Result"]

==== Wikipedia Visitor Counts

Let’s put Pig to a sterner test. Here’s the script above, modified to run on the much-larger Wikipedia dataset, and this time to assemble counts by hour:

.Pig - Wikipedia Page Views/Total Bytes by Project (ch_03/wikipedia_page_counts.pig)
------
/* Wikipedia pagecounts data described at https://dumps.wikimedia.org/other/pagecounts-raw/
   The first column is the project name. The second column is the title of the page retrieved, 
   the third column is the number of requests, and the fourth column is the size of the content returned. */
   
-- LOAD the data, which is space-delimited
pageviews = LOAD '/data/rawd/wikipedia/page_counts/pagecounts-20141126-230000.log' USING PigStorage(' ') AS (
   project_name:chararray, 
   page_title:chararray, 
   requests:long, 
   bytes:long
);

-- Group the data by project name, and then get counts of total pageviews and bytes sent for each project
per_project_counts = FOREACH (GROUP pageviews BY project_name) GENERATE
  group AS project_name, 
  SUM(pageviews.requests) AS total_pageviews, 
  SUM(pageviews.bytes) AS total_bytes;
-- Order the output by the total pageviews, in descending order
sorted_per_project_counts = ORDER per_project_counts BY total_pageviews DESC;
-- Store the data in our home directory
STORE sorted_per_project_counts INTO 'sorted_per_project_counts.out';

/*
LOAD SOURCE FILE
GROUP BY PROJECT NAME
SUM THE PAGE VIEWS AND BYTES FOR EACH PROJECT
ORDER THE RESULTS BY PAGE VIEWS, HIGHEST VALUE FIRST
STORE INTO FILE
*/
------

Run the script just as you did above:

------
hadoop fs -cat sorted_per_project_counts.out/* | head -10
------

Which should result in a top ten list of Wikipedia projects by page views:

------
meta.m	14163318	42739631770
en	8464555	271270368044
meta.mw	8070652	10197686607
en.mw	4793661	113071171104
es	2105765	48775855730
ru	1198414	38771387406
es.mw	967440	16660332837
de	967435	20956877209
fr	870142	22441868998
pt	633136	16647117186
------

Until now, we have described Pig as authoring the same Map/Reduce job you would. In fact, Pig has automatically introduced the same optimizations an advanced practitioner would have introduced, but with no effort on your part. Pig instructed Hadoop to use a Combiner. In the naive Python job, every Mapper output record was sent across the network to the Reducer but in Hadoop, as you will recall from (REF), the Mapper output files have already been partitioned and sorted. Hadoop offers you the opportunity to do pre-Aggregation on those groups. Rather than send every record for, say, September 26, 2014 8 pm, the Combiner outputs the hour and sum of visits emitted by the Mapper.

The second script instructed Pig to explicitly sort the output by total page views or requests, an additional operation. We did not do that in the first example to limit it to a single job. As you will recall from the previous chapter (REF), Hadoop uses a Sort to prepare the Reducer groups, so its output was naturally ordered. If there are multiple Reducers, however, that would not be enough to give you a Result file you can treat as ordered. By default, Hadoop assigns partitions to Reducers using the ‘RandomPartitioner’, designed to give each Reducer a uniform chance of claiming any given partition. This defends against the problem of one Reducer becoming overwhelmed with an unfair share of records but means the keys are distributed willy-nilly across machines. Although each Reducer’s output is sorted, you will see early records at the top of each result file and later records at the bottom of each result file.

What we want instead is a total sort, the earliest records in the first numbered file in order, the following records in the next file in order, and so on until the last numbered file. Pig’s ‘ORDER’ Operator does just that. In fact, it does better than that. If you look at the Job Tracker Console, you will see Pig actually ran three Map/Reduce jobs. As you would expect, the first job is the one that did the grouping and summing and the last job is the one that sorted the output records. In the last job, all the earliest records were sent to Reducer 0, the middle range of records were sent to Reducer 1 and the latest records were sent to Reducer 2.

Hadoop, however, has no intrinsic way to make that mapping happen. Even if it figured out, say, that the earliest buckets were sooner and the latest buckets were later, if we fed it a dataset with skyrocketing traffic in 2014, we would end up sending an overwhelming portion of results to that Reducer. In the second job, Pig sampled the set of output keys, brought them to the same Reducer, and figured out the set of partition breakpoints to distribute records fairly.

In general, Pig offers many more optimizations beyond these and we will talk more about them in the chapter on Advanced Pig (REF). In our experience, as long as you're willing to give Pig a bit of coaching, the only times it will author a dataflow that is significantly less performant comes when Pig is _overly_ aggressive about introducing an optimization. And in those cases the impact is more like a bunch of silly piglets making things take 50% longer than they should, rather than a stampede of boars blowing up your cluster. The `ORDER BY` example is a case in point: for small-to-medium tables the intermediate sampling stage to calculate partitions can have a larger time cost than the penalty for partitioning badly would carry. Sometimes you're stuck paying an extra 20 seconds on top of each one-minute job so that Pig and Hadoop can save you an order of magnitude off your ten-minute-and-up jobs.

=== Fundamental Data Operations

Pig's operators -- and fundamental Hadoop processing patterns -- can be grouped into several families: control operations, pipelinable operations, and structural operations.

A control operation either influences or describes the data flow itself. A pipelinable operation is one that does not require a reduce step of its own: the records can each be handled in isolation, and so they do not have to be expensively assembled into context. All structural operations must put records into context: placing all records for a given key into common context; sorting involves placing each record into context with the record that precedes it and the record that follows it; eliminating duplicates means putting all potential duplicates into common context, and so forth.

**Control Operations**

Control operations are essential to defining data-flows, or chains of data-processing.

* Serialization operations (LOAD, STORE) load and store data into file systems or datastores.
* Directives (DESCRIBE, ILLUSTRATE, REGISTER, and others) to Pig itself. These do not modify the data, they modify Pig's execution: outputting debug information, registering external UDFs, and so forth.

**Pipelinable Operations**

With no structural operations, these operations create a mapper-only job with the composed pipeline. When they come before or after a structural operation, they are composed into the mapper or reducer.

// TODO explain better

* Transformation operations (FOREACH, FOREACH..FLATTEN(tuple)) modify the contents of records individually. The count of output records is exactly the same as the count of input records, but the contents and schema of the records can change arbitrarily.
* Filtering operations (FILTER, SAMPLE, LIMIT, ASSERT) accept or reject each record individually. These can yield the same or fewer number of records, but each record has the same contents and schema as its input.
* Repartitioning operations (SPLIT, UNION) don't change records, they just distribute them into new tables or data flows. UNION outputs exactly as many records as the sum of its inputs. Since SPLIT is effectively several FILTERs run simultaneously, its total output record count is the sum of what each of its filters would produce.
* Ungrouping operations (FOREACH..FLATTEN(bag)) turn records that have bags of tuples into records with each such tuple from the bags in combination. It is most commonly seen after a grouping operation (and thus occurs within the Reduce) but can be used on its own (in which case like the other pipelinable operations it produces a Mapper-Only job). The FLATTEN itself leaves the bag contents unaltered and substitutes the bag field's schema with the schema of its contents. When flattening on a single field, the count of output records is exactly the count of elements in all bags. (Records with empty bags will disappear in the output). Multiple FLATTEN clauses yield a record for each possible combination of elements, which can be explosively higher than the input count.

**Structural Operations**

These jobs require a Map and Reduce phase.

* Grouping operations (GROUP, COGROUP, CUBE, ROLLUP) place records into context with each other. They make no modifications to the input records'  contents, but do rearrange their schema. You will often find them followed by a FOREACH that is able to take advantage of the group context. The GROUP and COGROUP yield one output record per distinct GROUP value.

* Joining operations (JOIN, CROSS) match records between tables. JOIN is simply an optimized COGROUP/FLATTEN/FOREACH sequence, but it is important enough and different in use that we'll cover it separately. (CROSS too, except for the "important" part: we'll have very little to say about it and discourage its use).

* Sorting operations (ORDER BY, RANK) perform a total sort on their input; every record in file 00000 is in sorted order and comes before all records in 00001 and so forth for the number of output files. These require two jobs:  first, a light Mapper-Only pass to understand the distribution of sort keys, next a Map/Reduce job to perform the sort.

* Uniquing and (DISTINCT, specific COGROUP forms) select/reject/collapse duplicates, or find records associated with unique or duplicated records.
these are typically accomplished with specific combinations of the above, but involve

// ... Composed to advanced patterns: Set Operations, Graph operations, ...

That's everything you can do with Pig -- and everything you need to do with data. Each of those operations leads to a predictable set of map and reduce steps, so it's very straightforward to reason about your job's performance. Pig is very clever about chaining and optimizing these steps. For example, a GROUP followed by a FOREACH and a FILTER will only require one map phase and one reduce phase. In that case, the FOREACH and FILTER will be done in the reduce step -- and in the right circumstances, pig will "push" part of the `FOREACH` and `FILTER` _before_ the `JOIN`, potentially eliminating a great deal of processing.

Pig is an extremely sparse language. By having very few Operators and very uniform syntax footnote:[Something SQL users but non-enthusiasts like your authors appreciate.], the language makes it easy for the robots to optimize the dataflow and for humans to predict and reason about its performance.

We will not explore every nook and cranny of its syntax, only illustrate its patterns of use. The online Pig manual at http://pig.apache.org/ is quite good and for a deeper exploration, consult _Programming Pig_ by Alan Gates (http://shop.oreilly.com/product/0636920018087.do). If the need for a construction never arose naturally in a pattern demonstration or exploration footnote:[An example of the first is `UNION ONSCHEMA` -- useful but not used.], we omitted it, along with options or alternate forms of construction that are either dangerous or rarely-used footnote:[it's legal in Pig to load data without a schema -- but you shouldn't, and so we're not going to tell you how.].

In the remainder of this chapter, we'll illustrate the mechanics of using Pig and the essential of its control flow operations by demonstrating them in actual use. In the following several chapters (REF), we'll cover patterns of pipelinable and of structural operations. In each case the goal is not only to understand its use, but to understand how to implement the corresponding patterns in a plain map-reduce approach -- and therefore how to reason about their performance. Finally, the chapter on Advanced Pig (TODO ref) will cover some deeper-level topics, such as a few important optimized variants of the JOIN statement and how to extend Pig with new functions and loaders.


=== `LOAD` Locates and Describes Your Data

Pig scripts need data to process, and so your pig scripts will begin with a LOAD statement and have one or many STORE statements throughout. Here's a script to find all wikipedia articles that contain the word 'Hadoop':

.Pig - LOADing, FILTERing and STORing Data (ch_03/load_filter_store.pig)
------
games = LOAD '/data/gold/sports/baseball/games_lite.tsv' AS (
  game_id:chararray,      year_id:int,
  away_team_id:chararray, home_team_id:chararray,
  away_runs_ct:int,       home_runs_ct:int
);

home_wins = FILTER games BY home_runs_ct > away_runs_ct;
STORE home_wins INTO './home_wins.tsv';
------

==== Simple Types ====

As you can see, the `LOAD` statement not only tells pig where to find the data, it also describes the table's schema. Pig understands ten kinds of simple type. Six of them are numbers: signed machine integers, as `int` (32-bit) or `long` (64-bit); signed floating-point numbers, as `float` (32-bit) or `double` (64-bit); arbitrary-length integers as `biginteger`; and arbitrary-precision real numbers, as `bigdecimal`. If you're supplying a literal value for a long, you should append a capital 'L' to the quantity: `12345L`; if you're supplying a literal float, use an 'f': `123.45f`.

The `chararray` type loads text as UTF-8 encoded strings (the only kind of string you should ever traffic in). String literals are contained in single quotes -- `'hello, world'`. Regular expressions are supplied as string literals, as in the example above: `'.*[Hh]adoop.*'`. The `bytearray` type does no interpretation of its contents whatsoever, but be careful -- the most common interchange formats (`tsv`, `xml` and `json`) cannot faithfully round-trip data that is truly freeform.

Lastly, there are two special-purpose simple types. Time values are described with `datetime`, and should be serialised in the the ISO-8601 format: `1970-01-01T00:00:00.000+00:00`. Boolean values are described with `boolean`, and should bear the values `true` or `false`.

// ===== Choosing a data type

Boolean, date and the biginteger/bigdecimal types are recent additions to Pig, and you will notice rough edges around their use.

// We most often represent boolean fields C-style: as an `int` holding 0 or 1.
// Dates
// TODO between here and the part of the FOREACH section where we assemble a complex type, give our policy on dates and booleans. As we do, make sure that we really feel that way about bools

==== Complex Type 1, Tuples: Fixed-length Sequence of Typed Fields

Pig also has three complex types, representing collections of fields. A `tuple` is a fixed-length sequence of fields, each of which has its own schema. They're ubiquitous in the results of the various structural operations you're about to learn. We usually don't serialize tuples, but so far `LOAD` is the only operation we've taught you, so for pretend's sake here's how you'd load a listing of major-league ballpark locations:

------
    -- The address and geocoordinates are stored as tuples. Don't do that, though.
    ballpark_locations = LOAD 'ballpark_locations' AS (
        park_id:chararray, park_name:chararray,
        address:tuple(full_street:chararray, city:chararray, state:chararray, zip:chararray),
        geocoordinates:tuple(lng:float, lat:float)
    );
    ballparks_in_texas = FILTER ballpark_locations BY (address.state == 'TX');
    STORE ballparks_in_texas INTO '/tmp/ballparks_in_texas.tsv'
------

Pig displays tuples using parentheses: it would dump a line from the input file as `BOS07,Fenway Park,(4 Yawkey Way,Boston,MA,02215),(-71.097378,42.3465909)'. As shown above, you address single values within a tuple using `tuple_name.subfield_name` -- for example, `address.state` will have the schema `state:chararray`. You can also create a new tuple that projects or rearranges fields from a tuple by writing `tuple_name.(subfield_a, subfield_b, ...)` -- for example, `address.(zip, city, state)` will have schema `address_zip_city_state:tuple(zip:chararray, city:chararray, state:chararray)`. (Pig helpfully generated a readable name for the tuple).

Tuples can contain values of any type, even bags and other tuples, but that's nothing to be proud of. We follow almost every structural operation with a `FOREACH` to simplify its schema as soon as possible, and so should you -- it doesn't cost anything and it makes your code readable.

==== Complex Type 2, Bags: Unbounded Collection of Tuples

A `bag` is an arbitrary-length collection of tuples, all of which are expected to have the same schema. Just like with tuples, they're ubiquitous yet rarely serialized. Again for pretend's sake we can load a dataset listing for each team the year and park id of the ballparks it played in:

------
    team_park_seasons = LOAD 'team_parks' AS (
        team_id:chararray,
        park_years: bag{tuple(year:int, park_id:chararray)}
        );
------

You can also address values within a bag using `bag_name.(subfield_a, subfield_b)`, but this time the result is a bag with the given projected tuples. You'll see examples of this shortly when we discuss `FLATTEN` and the various group operations. Note that the _only_ type a bag holds is tuple, even if there's only one field -- a bag of just park ids would have schema `bag{tuple(park_id:chararray)}`.

==== Complex Type 3, Maps: Collection of Key-Value Pairs for Lookup

Pig offers a `map` datatype to represent a collection of key-value pairs. The only context we've seen them used is for loading JSON data. A tweet from the twitter firehose has a sub-hash holding info about the user; the following snippet loads raw JSON data, immediately fixes the schema, and then describes the new schema to you:

------
REGISTER piggybank.jar
raw_tweets = LOAD '/tmp/tweets.json' USING org.apache.pig.piggybank.storage.JsonLoader(
             'created_at:chararray, text:chararray, user:map[]');
------

A `map` schema is described using square brackets: `map[value_schema]`. You can leave the value schema blank if you supply one later (as in the example that follows). The keys of a map are _always_ of type chararray; the values can be any simple type. Pig renders a map as `[key#value,key#value,...]`: Flip's twitter user record as a hash would look like `[name#Philip (flip) Kromer,id#1554031,screen_name#mrflip]'.

Apart from loading complex data, the `map` type is surprisingly useless. You might think it would be useful to carry around a lookup-table in a map field -- a mapping from ids to names, say -- and then index into it using the value of some other field, but a) you cannot do so and b) it isn't useful. The only thing you can do with a `map` field is dereference by a constant string, as we did above (`user#'id'`). Carrying around such a lookup table would be kind of silly, anyway, as you'd be duplicating it on every row. What you most likely want is either an off-the-cuff UDF or to use Pig's "replicated" `JOIN` operation; both are described in the chapter on Advanced Pig (TODO ref).

==== Defining the Schema of a Transformed Record

Since the map type is mostly useless, we'll seize the teachable moment and use this space to illustrate the other way schema are constructed: using a `FOREACH`. As always when given a complex schema, we took the first available opportunity to simplify it. The `FOREACH` in the snippet above dereferences the elements of the user `map` and supplies a schema for each new field with the `AS <schema>` clauses. The `DESCRIBE` directive that follows causes Pig to dump the schema to console: in this case, you should see `tweets: {created_at: chararray,text: chararray,user_id: long,user_name: chararray,user_screen_name: chararray}`.

------
REGISTER piggybank.jar
raw_tweets = LOAD '/tmp/tweets.json' USING org.apache.pig.piggybank.storage.JsonLoader(
             'created_at:chararray, text:chararray, user:map[]');
tweets = FOREACH raw_tweets GENERATE
        created_at,
        text,
        user#'id' AS user_id:long,
        user#'name' AS user_name:chararray,
        user#'screen_name' AS user_screen_name:chararray;
DESCRIBE tweets;
------

In the chapter on Advanced Pig (REF), we'll cover some further topics: loading from alternate file formats or from databases; how Pig and Hadoop assign input file splits to mappers; and custom load/store functions.

=== `STORE` Writes Data to Disk

The STORE operation writes your data to the destination you specify (typically the HDFS).

------
articles = LOAD './data/wp/articles.tsv' AS (page_id: long, namespace: int, wikipedia_id: chararray, revision_id: long, timestamp: long, title: chararray, redirect: chararray, text: chararray);
hadoop_articles = FILTER articles BY matches('.*[Hh]adoop.*');
STORE hadoop_articles INTO './data/tmp/hadoop_articles.tsv';
------

As with any Hadoop job, Pig creates a _directory_ (not a file) at the path you specify; each task generates a file named with its task ID into that directory. In a slight difference from vanilla Hadoop, If the last stage is a reduce, the files are named like `part-r-00000` (`r` for reduce, followed by the task ID); if a map, they are named like `part-m-00000`.

Try removing the STORE line from the script above, and re-run the script. You'll see nothing happen! Pig is declarative: your statements inform Pig how it could produce certain tables, rather than command Pig to produce those tables in order.

[[checkpointing_your_data]]
The behavior of only evaluating on demand is an incredibly useful feature for development work. One of the best pieces of advice we can give you is to checkpoint all the time. Smart data scientists iteratively develop the first few transformations of a project, then save that result to disk; working with that saved checkpoint, develop the next few transformations, then save it to disk; and so forth. Here's a demonstration:

------
    great_start = LOAD '...' AS (...);
    -- ...
    -- lots of stuff happens, leading up to
    -- ...
    important_milestone = JOIN [...];

    -- reached an important milestone, so checkpoint to disk.
    STORE important_milestone INTO './data/tmp/important_milestone';
        important_milestone = LOAD './data/tmp/important_milestone' AS (...schema...);
------

// TODO cover the STORE_TABLE / LOAD_RESULT macros

In development, once you've run the job past the `STORE important_milestone` line, you can comment it out to make pig skip all the preceding steps -- since there's nothing tying the graph to an output operation, nothing will be computed on behalf of `important_milestone`, and so execution will start with the following `LOAD`. The gratuitous save and load does impose a minor cost, so in production, comment out both the `STORE` and its following `LOAD` to eliminate the checkpoint step.

These checkpoints bring another benefit: an inspectable copy of your data at that checkpoint. Many newcomers to Big Data processing resist the idea of checkpointing often. It takes a while to accept that a terabyte of data on disk is cheap -- but the cluster time to generate that data is far less cheap, and the programmer time to create the job to create the data is most expensive of all. We won't include the checkpoint steps in the printed code snippets of the book, but we've left them in the example code.

=== Development Aids: `DESCRIBE`, `ASSERT`, `EXPLAIN`, `LIMIT..DUMP`, `ILLUSTRATE`

==== `DESCRIBE` shows the schema of a table

You've already seen the `DESCRIBE` directive, which writes a description of a table's schema to the console. It's invaluable, and even as your project goes to production you shouldn't be afraid to leave these statements in where reasonable.

==== `ASSERT` checks that your data is as you think it is

The `ASSERT` operation applies a test to each record as it goes by, and fails the job if the test is ever false. It doesn't create a new table, or any new map/reduce passes -- it's slipstreamed into whatever operations precede it -- but it does cause per-record work. The cost is worth it, and you should look for opportunities to add assertions wherever reasonable.

==== `DUMP` shows data on the console with great peril

The `DUMP` directive is actually equivalent to `STORE`, but (gulp) writes its output to your console. Very handy when you're messing with data at your console, but a trainwreck when you unwittingly feed it a gigabyte of data. So you should never use a `DUMP` statement except as in the following stanza: `dumpable = LIMIT table_to_dump 10; DUMP dumpable;`.
(ATTN tech reviewers: should we even discuss `DUMP`? Is there a good alternative, given `ILLUSTRATE`s flakiness?)

// TODO sh head?
// sh egrep -- note escaping -- include an || true

==== `ILLUSTRATE` magically simulates your script's actions, except when it fails to work

The `ILLUSTRATE` directive is one of our best-loved, and most-hated, Pig operations.
Even if you only want to see an example line or two of your output, using a `DUMP` or a `STORE` requires passing the full dataset through the processing pipeline. You might think, "OK, so just choose a few rows at random and run on that" -- but if your job has steps that try to match two datasets using a `JOIN`, it's exceptionally unlikely that any matches will survive the limiting. (For example, the players in the first few rows of the baseball players table belonged to teams that are not in the first few rows from the baseball teams table.)  `ILLUSTRATE` walks your execution graph to intelligently mock up records at each processing stage. If the sample rows would fail to join, Pig uses them to generate fake records that will find matches. It solves the problem of running on ad-hoc subsets, and that's why we love it.

However, not all parts of Pig's functionality work with ILLUSTRATE, meaning that it often fails to run. When is the `ILLUSTRATE` command is most valuable? When applied to less-widely-used operations and complex sequences of statements, of course. What parts of Pig are most likely to lack `ILLUSTRATE` support or trip it up? Well, less-widely-used operations and complex sequences of statements, of course. And when it fails, it does so with perversely opaque error messages, leaving you to wonder if there's a problem in your script or if `ILLUSTRATE` has left you short. If you, eager reader, are looking for a good place to return some open-source karma: consider making `ILLUSTRATE` into the tool it could be. Until somebody does, you should checkpoint often (described along with the `STORE` command above) and use the strategies for subuniverse sampling from the Statistics chapter (TODO ref).

Lastly, while we're on the subject of development tools that don't work perfectly in Pig: the Pig shell gets confused too easily to be useful. You're best off just running your script directly. For local mode development, tools like watchr (REF) can intelligently relaunch a script every time you hit save, streamlining development.

==== `EXPLAIN` shows Pig's execution graph

The `EXPLAIN` directive writes the "execution graph" of your job to the console. It's extremely verbose, showing _everything_ pig will do to your data, down to the typecasting it applies to inputs as they are read. We mostly find it useful when trying to understand whether Pig has applied some of the optimizations you'll learn about in Tuning for the Wise and Lazy (TODO ref). (QUESTION for tech reviewers: move this section to advanced Pig and explain EXPLAIN?)

=== Pig Functions act on fields

Pig wouldn't be complete without a way to _act_ on the various fields.  It offers a sparse but essential set of built-in functions -- the Pig cheatsheet (TODO ref) at the end of the book gives a full list. The whole middle of the book is devoted to examples of Pig and map/reduce programs in practice (and in particular a chapter on Statistics), so we'll just list the highlights here:

* *Math functions* for all the things you'd expect to see on a good calculator: `LOG`/`LOG10`/`EXP`, `RANDOM`, `ROUND`/’DOUND_TO`/`FLOOR`/`CEIL`, `ABS`, trigonometric functions, and so forth.
* *String comparison*:
 - `matches` tests a value against a regular expression:
 - Compare strings directly using `==`. `EqualsIgnoreCase` does a case-insensitive match, while `STARTSWITH`/`ENDSWITH` test whether one string is a prefix or suffix of the other.
 - `SIZE` returns the number of characters in a `chararray`, and the number of bytes in a `bytearray`. Be reminded that characters often occupy more than one byte: the string 'Motörhead' has nine characters, but because of its umlaut-ed 'ö' the string occupies ten bytes. You can use `SIZE` on other types, too; but to find the number of elements in a bag, use `COUNT_STAR` and not `SIZE`.
 - `INDEXOF` finds the character position of a substring within a `chararray`
* *Transform strings*:
 - `CONCAT` concatenates all its inputs into a new string; `SPRINTF` uses a supplied template to format its inputs into a new string; BagToString joins the contents of a bag into a single string, separated by a supplied delimiter
 - `LOWER` converts a string to lowercase characters; `UPPER` to all uppercase
 - `TRIM` strips leading and trailing whitespace
 - `REPLACE(string, 'regexp', 'replacement')` substitutes the replacement string wherever the given regular expression matches, as implemented by `java.string.replaceAll`. If there are no matches, the input string is passed through unchanged.
 - `REGEX_EXTRACT(string, regexp, index)` applies the given regular expression and returns the contents of the indicated matched group. If the regular expression does not match, it returns NULL. The `REGEX_EXTRACT_ALL` function is similar, but returns a tuple of the matched groups.
 - `STRSPLIT` splits a string at each match of the given regular expression
 - `SUBSTRING` selects a portion of a string based on position
* *Datetime Functions*, such as `CurrentTime`, `ToUnixTime`, `SecondsBetween` (duration between two given datetimes)
* *Aggregate functions* that act on bags:
 - `AVG`, `MAX`, `MIN`, `SUM`
 - `COUNT_STAR` reports the number of elements in a bag, including nulls; `COUNT` reports the number of non-null elements. `IsEmpty` tests that a bag has elements. Don't use the quite-similar-sounding `SIZE` function on bags: it's much less efficient.
* *Bag Functions* TODO COMPLETE LIST
  - Extremal
  - FirstTupleInBag
  - BagConcat
  - Stitch / Over
  - `SUBTRACT(bag_a, bag_b)` returns a new bag having all the tuples that are in the first but not in the second, and `DIFF(bag_a, bag_b)` returns a new bag having all tuples that are in either but not in both. These are rarely used, as the bags must be of modest size -- in general us an inner JOIN as described below.
  - `TOP(num, column_index, bag)` selects the top `num` of elements from each tuple in the given bag, as ordered by `column_index`. This uses a clever algorithm that doesn't require an expensive total sort of the data -- you'll learn about it in the Statistics chapter (TODO ref)
* *Conversion Functions* to perform higher-level type casting: `TOTUPLE`, `TOBAG`, `TOMAP`

* Add all the DataFu operations: http://datafu.incubator.apache.org/docs/datafu/guide/bag-operations.html and coalesce http://datafu.incubator.apache.org/docs/datafu/guide/more-tips-and-tricks.html and maybe others
  - Hasher


=== Moving right along …

This chapter was a gentle introduction to Pig and its basic operations.  In the next two chapters, we'll see Pig in action. footnote:[Your authors struggled to not make a "see Pig fly" joke. Oh, wait, looks like we did …]
