
* Storm Internals
  - Glossary of Terms
  - Tuple Life Cycle
  - Disruptor Queue
  - Transport of Tuples
  - Topology Life Cycle
* Trident
  - Trident motivation
  - Trident Fundamental Operations:
  - Trident Is Storm
  - Batch and Partition:
* Trident Transactionality
* Tuning Storm+Trident



Intro:
  - ???
=== Storm Internals


==== Glossary of Terms

===== Components

* **supervisor**
  - JVM process launched on each storm worker machine. Does not execute your code -- supervises it.
  - number of workers set by number of `supervisor.slots.ports`

* **worker**
  - jvm process launched by the supervisor
  - intra-worker transport is more efficient, so run one worker per topology per machine
  - if worker dies, supervisor will rest
  
* **Coordinator** generates new transaction ID
  - figures out what kafka hosts
  - sends tuple, which influences spout to dispatch a new batch
  - each transaction ID corresponds identically to single trident batch and vice-versa
  - Transaction IDs for a given topo_launch are serially incremented globally.
  - knows about Zookeeper /transactional; so it recovers the transaction ID

* **Kafka Spout** -- suppose 6 kafka spouts (3 per worker, 2 workers), reading from 24 partitions
  - each spout would ping 4 partitions assigned to it, pulling in `max_fetch_size` bytes from each: so we would get `12 * max_fetch_size` bytes on each worker, `24 * max_fetch_size` bytes in each batch
  - Each `adact` becomes one kafka message, which becomes exactly one tuple
  - In our case, incoming records are about 1000 bytes, and messages add a few percent of size. (4000 records takes 4_731_999 bytes, which fits in a 5_000_000 max_fetch_size request).
  - Each trident batch is assembled in parallel across all spouts
  - So trident batch size is
    - `spout_batch_kb     ~= max_fetch_size * kafka_machines * kpartitions_per_broker / 1024`
    - `spout_batch_tuples ~= spout_batch_kb * 1024 / bytes_per_record`
    - `record_bytes       ~= 1000 bytes`

* **Executor**
  - Each executor is responsible for one bolt
  - so with 3 kafka spouts on a worker, there are three executors spouting

=== Trident Lifecycle

==== Coordinator

* Master coordinator is secretly the spout
* at trident batch delay period, will emit a transaction tuple
* it has a serially incrementing transaction ID, kept forever even across restarts.
* (We're only going to talk about Opaque Transactional topologies; more on that later)

==== Spout

spout emits batches; these succeed or fail as a whole. (That's because they're part of the Storm tuple tree of the coordinator's seed tuple).

==== Processors

* tuples in batches are freely processed in parallel and asynchronously unless there are barriers (eg the state after the group by)
* In this case, Processor emits aggregable records



* __Bolt__ -- topology-level object.

  - contract:
    - `execute` is called
    - you may call `emit` zero one or many times,
    - and then you must call either `ack` or `fail`
    - so, execute method _must_ be synchronous (blocking calls). No fair suspending yourself and returning from execute for some later ackness or failness. That's Storm's job.
      - TODO: verify.

__Physical-level objects__

* __Supervisor__
  - hosts worker
  - has many workers
* __Worker__
  - has many executors, belongs to supervisor
  - role:
    - hosts zmq sockets
    - accepts inbound tuples from other workers (worker receive queue)
    - dispatches outbound tuples to other workers (worker transfer queue)
    - (other stuff)

* __Executors__
  - belongs to executor; has one bolt/spout
  - role:
    - accepts inbound tuples (executor receive queue)
    - dispatches outbound tuples (executor send queue)
  - each executor is one single thread
   - calls tasks serially
* __Tasks__ --
  - belongs to executor; has one bolt/spout
  - physical expression of the bolt or spout
  - in Storm, can set many tasks per executor -- when you want to scale out (TODO: verify). (in Trident, left at one per; TODO: can this be changed?)
==== Tuple Life Cycle 

(Reference later section on latency tolerance)
Disruptor Queue
      - A perfect intermediate buffering queue.
      - love how locked down and ungenerous, therefore simple and powerful it is
      - sits right before and right after every executor
      - main point: how the disruptor queue is swept

===== Queues

* executor send buffer
* executor receive buffer
* worker receive buffer
* worker transfer buffer
  
==== Storm Transport

Each executor (bolt or spout) has two disruptor queues: its 'send queue' (the individual tuples it emits) and its 'receive queue' (batches of tuples staged for processing)footnote:[It might seem odd that the spout has a receive queue, but much of storm's internal bookkeeping is done using tuples -- there's actually a regular amount of traffic sent to each spout].

===== Disruptor Queue

At the heart

===== Spout Tuple Handling

* If the spout executor's async-loop decides conditions are right, it calls the spout's `nextTuple()` method.
* The spout can then emit zero, one or many tuples, which the emitter publishes non-blocking into the spout's executor send queue (see below for details).
* Each executor send queue (spout or bolt) has an attached router (`transfer-fn`). In an infinite loop, it
  - lays claim to all messages currently in the queue (everything between its last-read position and the write head), and loads them into a local tuple-batch.
  - sorts tuples into two piles: local ones, destined for tasks on this worker; and remote ones, destined for tasks on other workers.
  - all the remote tuples are published (blocking) as a single batch into the worker's transfer queue; they'll be later sent over the network each to the appropriate worker
  - the router regroups the tuples by task, and publishes (blocking) each tuple-batch into that task's executor receive buffer.
  Note that the executor send queue holds individual _tuples_, where as the worker transfer queue and executor receive queues hold _collections of tuples_. An executor send queue size of 1024 slots with an executor receive queue size of 2048 slots means there won't ever be more than `2048 * 1024` tuples waiting for that executor to process. It's important also to recognize that, although the code uses the label `tuple-batch` for these collections of tuples, they have nothing to do with the higher-level concept of a 'Trident batch' you'll meet later.

===== Bolt Tuple Handling

...

===== Worker Transfer and Receive Handlers


Unlike the transfer and the executor queues, the worker's receive buffer is a ZeroMQ construct, not a disruptor queue


==== Transport of Tuples
diagram of a flow. Explain how tuples move through the flow
    - executor send/receive
    - worker transfer buffers
    - putting all those queues there sets a floor on how fast you can go.
      - if microseconds matter, storm will never be appropriate
    - which are qbunches and which are regular tuples

==== Acking and Reliability

Acking framework/machinery for guaranteeing at least once (maybe sidebar or advanced topic to upper section)


==== Acking In Storm

* Noah is processed, produces Ham and Shem. Ack clears Noah, implicates Ham and Shem
* Shem is processed, produces Abe. Ack clears Shem, implicates Abe
* Ham is processed, produces non;e. Ack clears Ham


* Alice does a favor for Bob and Charlie. Alice is now in the clear; Bob and Charlie owe
* 


* For every record generated, send it to the acker
* Who keeps it in a table
* For every record completed, send it to the acker
* Who removes it from the table
* Maintain tickets in a tree structure so you know what to retry

Instead,

* When the tuple tree is created, send an ack-init: the clan id along with its edge checksum
* When each tuple is successfully completed, send an ack holding two sixty-four bit numbers: the tupletree id, and the XOR of its edge id and all the edge ids it generated. Do this for each of its tupletree ids.
* The acker holds a single O(1) lookup table
    - it's actually a set of lookup tables: current, old and dead. new tuple trees are added to the current bucket; every timeout number of seconds, current becomes old, and old becomes dead -- they are declared failed and their records retried.
* The spout holds the original tuple until it receives notice from the acker. The spout won't fetch more than the max-pending number of tuples: this is to protect the spout against memory pressure , and the downstream system against congestion.



When a tuple is born in the spout,

* creates a `root-id` -- this will identify the tuple tree. Let's say it had the value `3`.
* for all the places the tuple will go, makes an `edge-id` (`executor.clj:465`)
  - set the ack tree as `{ root_id: edge_id }`. Say the tuple was to be sent to three places; it would call `out_tuple(... {3: 100})`, `out_tuple(... {3: 101})`, `out_tuple(... {3: 102})`.
* XORs all the edge_id's together to form a partial checksum: `100 ^ 101 ^ 102`.
* sends an `init_stream` tuple to the acker as `root_id, partial_checksum, spout_id`
* the tuple's `ack val` starts at zero.

When a tuple is sent from a bolt, it claims one or more anchors (the tuples it came from), and one or more destination task ids.


====== Acker Walkthrough

When a tuple is born in the spout,

* creates a `root-id` -- this will identify the tuple tree. Let's say it had the value `3`.
* for all the places the tuple will go, makes an `edge-id` (`executor.clj:465`)
  - set the ack tree as `{ root_id: edge_id }`. Say the tuple was to be sent to three places; it would call `out_tuple(... {3: 100})`, `out_tuple(... {3: 101})`, `out_tuple(... {3: 102})`.
* XORs all the edge_id's together to form a partial checksum: `100 ^ 101 ^ 102`.
* sends an `init_stream` tuple to the acker as `root_id, partial_checksum, spout_id`
* the tuple's `ack val` starts at zero.

When a tuple is sent from a bolt, it claims one or more anchors (the tuples it came from), and one or more destination task ids.

[[acker_lifecycle_simple]]
.Acker Lifecycle: Simple
[cols="1*<.<d,1*<.<d,1*<.<d",options="header"]
|=======
| Event				 	| Tuples			    	| Acker Tree
| spout emits one tuple to bolt-0 	| noah:   `<~,     { noah: a  }>`   	|
| spout sends an acker-init tuple, seeding the ack tree with `noah: a`
                                       	|                                 	| `{ noah: a }`
| bolt-0 emits two tuples to bolt-1 anchored on `noah`. Those new tuples each create an edge-id for each anchor, which is XORed into the anchor's `ackVal` and used in the new tuple's message-id.
                                        | shem: `<~,       { noah: b  }>` + 
                                          ham:  `<~,       { noah: c  }>` + 
                                          noah: `<b^c,     { noah: a  }>` 	|
| bolt-0 acks acks `noah` using the XOR of its ackVal and tuple tree: `noah: a^b^c`. Since `a^a^b^c = b^c`, this clears off the key `a`, but implicates the keys `b` and `c` -- the tuple tree remains incomplete.
                                      	|                                    	| `{ noah: b^c }`
| bolt-1 processes `shem`, emits `abe` to bolt-2
                                       	| abe:    `<~,     { noah: d  }>` + 
                                     	  shem:   `<d,     { noah: b  }>`  	|
| bolt-1 acks `shem` with `noah: d^b`  	|                                      	| `{ noah: c^d }`
| bolt-1 processes `ham`, emits nothing	| ham:    `<~,     { noah: c  }>`	|
| bolt-1 acks `ham` with `noah: c`   	|                                   	| `{ noah: d }`
| bolt-1 processes `abe`, emits nothing	| abe:    `<~,     { noah: d  }>`	|
| bolt-1 acks `abe` with `noah: d`	|                                  	| `{ noah: 0 }`
| acker removes noah from ledger, notifies spout
                                        |                                    	| `{}`
|	|	|
| `______________________`            	| `______________________________`	| `___________________`
|=======

We have one tuple, with many anchors, to many out-task ids.

----
    hera ----v---- zeus ----v---- dione
             |              |
            ares ---v--- aphrodite
                    |
           +--------+--------+
        phobos   deimos   harmonia
----


traffic occurs to the acker in two places:

* each time a spout emits a tuple
* each time a bolt acks a tuple

even if there are thousands of tuples, only a very small amount of data is sent: the init_stream when the tuple tree is born, and once for each child tuple.
When a tuple is acked, it both clears its own record and implicates its children.

====== Acker

* Acker is just a regular bolt -- all the interesting action takes place in its execute method.
* it knows
  - id == `tuple[0]` (TODO what is this)
  - the tuple's stream-id
  - there is a time-expiring data structure, the `RotatingHashMap`
    - it's actually a small number of hash maps;
    - when you go to update or add to it, it performs the operation on the right component HashMap.
    - periodically (when you receive a tick tuple), it will pull off oldest component HashMap, mark it as dead; invoke the expire callback for each element in that HashMap.
* get the current checksum from `pending[id]`.

pending has objects like `{ val: "(checksum)", spout_task: "(task_id)" }`

* when it's an ACKER-INIT-STREAM
  `pending[:val] = pending[:val] ^ tuple[1]`
*


pseudocode

    class Acker < Bolt

	def initialize
	  self.ackables = ExpiringHash.new
	end

  	def execute(root_id, partial_checksum, from_task_id)
	  stream_type = tuple.stream_type
	  ackables.expire_stalest_bucket if (stream_type == :tick_stream)
	  curr = ackables[root_id]

	  case stream_type
	  when :init_stream
	    curr[:val]        = (curr[:val]	|| 0) ^ partial_checksum
	    curr[:spout_task] = from_task_id
	  when :ack_stream
	    curr[:val]        = (curr[:val]	|| 0) ^ partial_checksum
	  when :fail_stream
	    curr[:failed]     = true
	  end

	  ackables[root_id] = curr

	  if    curr[:spout_task] && (curr[:val] == 0)
	    ackables.delete(root_id)
	    collector.send_direct(curr[:spout_task], :ack_stream, [root_id])
	  elsif curr[:failed]
	    ackables.delete(root_id)
	    collector.send_direct(curr[:spout_task], :fail_stream, [root_id])
	  end

	  collector.ack # yeah, we have to ack as well -- we're a bolt
	end

    end






====== A few details

There's a few details to clarify:

First, the spout must never block when emitting -- if it did, critical bookkeeping tuples might get trapped, locking up the flow. So its emitter keeps an "overflow buffer", and publishes as follows:

* if there are tuples in the overflow buffer add the tuple to it -- the queue is certainly full.
* otherwise, publish the tuple to the flow with the non-blocking call. That call will either succeed immediately ...
* or fail with an `InsufficientCapacityException`, in which case add the tuple to the overflow buffer

The spout's async-loop won't call `nextTuple` if overflow is present, so the overflow buffer only has to accomodate the maximum number of tuples emitted in a single `nextTuple` call.



Topology Life Cycle
from launch to running`
    - you upload your jar, nimbus instantiates your topoology, then it goes to each supervisor, child processes evaluate topology, etc...

=== Trident

==== Trident motivation
  - example of things you can't do with storm
    - what topologies could we run that wouldn't work on storm?

==== Trident Fundamental Operations:
  - https://github.com/nathanmarz/storm/wiki/Trident-API-Overview

  - each: accepts one tuple, emits 0, 1, or many tuples
  - filter: accepts one tuple, implements isKeep, causing it to emit 0 or 1 tuples
  - batch query/batch apply/map
    - Accepts a list of n tuples
    - emits a list of n things (could be tuples, objects, whatever)
    - then, in 'executor' the list of n things is handed to a method that emits 0, 1 or many tuples for each thing
  - batch apply/map
    - emits only 1 thing for each of the n things
    - list of results
  - batch each
    - emits the original n things
    - list of result lists
  - partition persist
    - basically a batch query with a commit guarantee
    - equivalent of pig store
    - talk about failure cases
      - what happens if some of the partitions fail in the commit? where are the tuples retried from?
    - school bus metaphor:
      - there is no ordering of partitions, however there is an ordering over tuples within partitions
       - you will never see an out of order tuple within a partition, but you might switch contexts between partitions
         - i.e: Partition Green - 1,2,3,4,5 Parition Red: 2,3,4,5 Partition Green: 6,7,8,9,10
      - coord tuple is at the end of the partition
  - persistent aggregate
    - a persistent aggregate ensures that an aggregator is applied to each tuple in a batch, and persists the result
      using transactional guarantees
    - commits once the prior batch has succeeded and all partitions have succeeded(completed)
  - partition aggregate
    - does not have a commit
    - give it an aggregator function, ensuring that the aggregator is applied to each partition in the batch
    - like persistent aggregate without commit

Trident aggregators
    - aggregator (cannot be used with a persist)
      - gets an init call, can emit
      - gets a call with aggregate with every tuple in a partition, can emit
      - gets a call to complete, can emit
    - accumulator
      - calls firstTuple() with the first tuple in a partition
      - calls aggregate method with each tuple in partition, including first
      - calls complete
    - improver * the method signatures may change slightly
      - accepts a unified profile and deltas, emits 'improved' profile
      - complete called, emits 'posterior'
    - reducing aggregator (fold)
      - call init (sometimes) for first element in series
      - stepwise improver - one at a time. Improver accepts many deltas, reducing aggregator accepts one at a time
    - combiner
      - has a zero method
      - has a combine method that accepts to values and returns combined value

Trident Persistent Aggregates

reducer aggregator:
will be called for each element in the group with the prior value and the tuple
the Trident `State` has retrieved the prior values for the keys
the Trident `State` is given the new tuples 
Persistent Aggregate has the following things
aggregator, which is called with the list of values (refer above)
there is a backing store facade which presents the interface for a key-value store
    - it’s a key-value store of some kind. Concrete implementations exist for memory, elasticsearch, memcached, hbase, etc…
    - can say getValues and give it a bunch of group keys
    - can say putValues and give it a list of group keys and a list of group values

Persistent aggregate has many moving parts that are all decomposed and connected. we are *not* going to describe every single way to assemble a persistent aggregate. We will give you the (somewhat elaborate) recommended configuration that you will want to use most of the time, but will not go into all of the details of constructing your own from scratch.
 
Recommended Pieces:
Data Store Adapter (look up correct word) presents a map-esque interface that actually carries out storing and retrieving values.
Write-through LRU cache sitting in front of data store adapter -- wraps the data store adapter with an LRU cache
Aggregator function like those listed above that performs the aggregation 
Not covered:
Serializers  - responsible for turning a value into a string suitable to be sent to the datastore
Value Updater - responsible for something *look up function*
State Updater - also does something *look up function*


Life Cycle:

In case you’re wondering we’re going to talk about a transactional topology with a non-combiner aggregator first. We’ll explain more later. The point being that this aggregator must get the values before performing the aggregation.

1) Executor hosting this state operation enters the commit phase (commit tuple arrives)
2) Executor calls getValues() on the datastore (actually lru cache) on all group keys within the partition. The values are retrieved for use in the aggregation. e.g. in the diagram for the first partition, those keys will a,e,l,m,o,u,t. Note that even though l occurs twice it’s only one key.
3) Executor applies aggregator to each group (i.e. the values associated with the keys aelmout)

In the letter counting example the first executor in batch two gets the keys a,c, and e. The data store has values for a and e from the first batch. Next it will apply the aggregator function. In the case of key a, it will call reduce() 3 times: call it with 
==== Trident Is Storm

  - describe the storm topology that results from a trident topology
    - lay of the land/actors

==== Batch and Partition:
  - implications of transactions (transactional guarantees)/lifecycle of a batch/partition
    - there is no mechanism guaranteeing a partial order
    - each partition becomes complete once 
      - field-trip metaphor
    - guarantee is this: if you are told to commit, you are guaranteed that all batches before you have succeeded 
      and no batch after you has tried to commit.



==== Trident Transactionality

  - A detailed look at partitions and transactions
    - partition = sub-batch
    - set of things that should all be committed to the database at the same time
    - partitions are defined back at the trident. they're where you actually save something
    - partition/persist is an example of what exactly once is used for

===== Kinds of State

* non-transactional: batching behavior only
* transactional: exactly once; batches are always processed in whole
* opaque transactional: all records are processed, but might not be in same batches


==== Transactional State

* the state doesn't ask the cache to fetch until it has a whole batches' worth of records to hand over. This is trident logic, not storm.
* Those "aggregables" are reduced into rolled-up aggregates. So you might have 2500 inbound records that result in 900 distinct aggregates. (If you had eight aggregables [A, A, C, A, B, D, B, A] you would get four partial aggregates {A: 4, B: 2, C: 1, D: 1}. 
* It's clever about doing partial aggregates ("algebraic" reducers).


* It looks in the cache for the old total count. Anything that isn't there it fetches from the database. This lets you do efficient batch requests, a huge scalability boon.
* Once the cache is fresh, it determines the next aggregated value and writes it to the cache and to the DB, then ack()s the batch (all the tuples in the batch, really).
* If a batch had 900 aggregates, and it had prior counts for 250 of them, then it will _read_ 650 records and _write_ 900. It always does a put for every new observed count.

* ¡Note!: The database writes do *not* have to be transactional. It's the whole thing -- the whole batch, end-to-end -- that has to have transactional integrity, not just the DB code.

===== Ensuring Transactional reliability

Let's say for transaction ID 69 the old aggregated values were `{A:20, B: 10, C: 1, D: 0}`, and new  aggregated values were `{A: 24, B: 12, C: 2, D: 1}`. 

It stores (TODO: verify order of list):

   {A: [24, 20, 69], B: [12, 10, 69], C: [2, 1, 69], D: [1, 0, 69]}

If I am processing batch 

Since this is a _State_, you have contractual obligation from Trident that batch 69 will *not* be processed until and unless batch 68 has succeeded. 

So when I go to read from the DB, I will usually see something like

   {A: [20, ??, 68], B: [10, ??, 68], C: [1, ??, 68]}

I might instead however see

  {A: [??, 20, 69], B: [??, 10, 69], C: [??, 1, 69], D: [??, 0, 69]}

This means another attempt has been here: maybe it succeeded but was slow; maybe it failed; maybe _I_ am the one who is succeeding but slow. In any case, I don't know whether to trust the _new_ (first slot) values for this state, but I do know that I can trust the prior (second slot) values saved from batch 68. I just use those, and clobber the existing values with my new, correct counts.

==== Code Locations


====== Code Locations

Since the Storm+Trident code is split across multiple parent directories, it can be hard to track where its internal logic lives. Here's a guide to the code paths as of version `0.9.0-wip`.

[[storm_transport_code]]
.Storm Transport Code
|=======
| Role			 	| source path				    	|
| `async-loop`		 	| `clj/b/s/util.clj`		    	|
| Spout instantiation	 	| `clj/b/s/daemon/executor.clj`  	| `mk-threads :spout`
| Bolt instantiation	 	| `clj/b/s/daemon/executor.clj`  	| `mk-threads :bolt`
| Disruptor Queue facade 	| `clj/b/s/disruptor.clj` and `jvm/b/s/utils/disruptor.java`  	|
| Emitter->Send Q logic	 	| `clj/b/s/daemon/executor.clj`  	| `mk-executor-transfer-fn`
| Router (drains exec send Q)	| `clj/b/s/daemon/worker.clj`	    	| `mk-transfer-fn`	| infinite loop attached to each disruptor queue
| Local Send Q -> exec Rcv Q 	| `clj/b/s/daemon/worker.clj`	    	| `mk-transfer-local-fn`	| invoked within the transfer-fn and receive thread
| Worker Rcv Q -> exec Rcv Q 	| `clj/b/s/messaging/loader.clj` 	| `launch-receive-thread!`	| Worker Rcv Q -> exec Rcv Q
| Trans Q -> zmq	 	| `clj/b/s/daemon/worker.clj`	    	| `mk-transfer-tuples-handler`
| `..`			 	| `clj/b/s/daemon/task.clj`	    	|
| `..`			 	| `clj/b/s/daemon/acker.clj`	    	|
| `..`			 	| `clj/b/s/`			    	|
|=======


==== Tuning Storm and Trident


===== Numerology

The following should be even multiples:

* `N_w` workers per machine. (one if you're only running one topology)
* `N_spouts` per
  - `N_partitions_per_spout` -- even number of partitions per spout
  
* Don't change multiplicity lightly
  - it will route directly
  - don't really understand how/when/why yet


* Parallelism hint is a hint --
  - can get more never less (TODO: verify)



=== Intro to Trident (appears earlier) 
hey lets use storm a bunch


==== Storm+Trident: Lifecycle of a Record

===== Refs

* http://www.slideshare.net/lukjanovsv/twitter-storm?from_search=1


===== notes for genealogy analogy

http://www.theoi.com/Text/Apollodorus1.html [1.1.1] Sky was the first who ruled over the whole world.1  ... 
[1.1.3] [Uranus] begat children by Earth, to wit, the Titans as they are named: Ocean, Coeus, Hyperion, Crius, Iapetus, and, youngest of all, Cronus; also daughters, the Titanides as they are called: Tethys, Rhea, Themis, Mnemosyne, Phoebe, Dione, Thia.5  ... 
[1.3.1] Now Zeus wedded Hera and begat Hebe, Ilithyia, and Ares,32 but he had intercourse with many women, both mortals and immortals. By Themis, daughter of Sky, he had daughters, the Seasons, to wit, Peace, Order, and Justice; also the Fates, to wit, Clotho, Lachesis, and Atropus33; by Dione he had Aphrodite34; by Eurynome, daughter of Ocean, he had the Graces, to wit, Aglaia, Euphrosyne, and Thalia35; by Styx he had Persephone36; and by Memory (Mnemosyne) he had the Muses, first Calliope, then Clio, Melpomene, Euterpe, Erato, Terpsichore, Urania, Thalia, and Polymnia.37
http://www.theoi.com/Text/HomerIliad5.html "Straightway then they came to the abode of the gods, to steep Olympus and there wind-footed, swift Iris stayed the horses and loosed them from the car, and cast before them food ambrosial; but fair Aphrodite flung herself upon the knees of her mother Dione. She clasped her daughter in her arms, and stroked her with her hand and spake to her, saying: "Who now of the sons of heaven, dear child, hath entreated thee thus wantonly, as though thou wert working some evil before the face of all?""
http://www.maicar.com/GML/OCEANIDS.html Dione 1. Dione 1 ... daughter of Uranus & Gaia. According to some she consorted with Zeus and gave birth to Aphrodite. Apd.1.1.3, 1.3.1; Hom.Il.5.370; Hes.The.350ff
http://www.maicar.com/GML/Aphrodite.html Aphrodite had three children by Ares: Deimos, Phobus 1 (Fear and Panic) and Harmonia 1
http://www.theoi.com/Text/HesiodTheogony.html Hesiod [933] Also Cytherea bare to Ares the shield-piercer Panic and Fear, terrible gods who drive in disorder the close ranks of men in numbing war, with the help of Ares, sacker of towns: and Harmonia whom high-spirited Cadmus made his wife.

